{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Depedencies"
      ],
      "metadata": {
        "id": "KcwFwvrNqkmE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qpWZF8uZbCz"
      },
      "outputs": [],
      "source": [
        "!pip install -q nltk\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3bYxCXTjqtX"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n",
        "# Overview for our first architecture - NLP Without Neural Network\n",
        "\n",
        "For our first architecture we'll learn how to do sentiment classification using \"Naive Bayes\". We'll delve in several key concepts of NLP that will help us later understand more complex architecture like Seq2Seq and Transformers such as Preprocessing, Word Embedding, tokenization, and more!\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_R2iHDwSe_.png?updatedAt=1695078679500)\n",
        "\n",
        "As you can see on above diagram, we'll learn the steps of asking, converting that question to numbers, and then making sure our model understand that question. For modern architecture the concept of understanding is mostly using an encoder layer, but for methods like \"Naive Bayes\" it's kinda like encoder layer, but much more traditional than that.\n",
        "\n",
        "> Note: \"Naive Bayes\" method will be covered in the next lesson. Look forward to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO1KxMVyfgCY"
      },
      "source": [
        "# Text classification\n",
        "\n",
        "One of the task that we can use for NLP without neural network is \"Text classification\". This task is as simple as it sound: What current input should be classified to?\n",
        "\n",
        "For today we'll learn how to classify a tweet if it can be count as positive tweet or negative.\n",
        "\n",
        "Imagine this tweet:\n",
        "\n",
        "> I'm really excited towards tomorrow for our shop opening, see you guys at xxx!\n",
        "\n",
        "We as a human can know from above tweet that the person who tweeted currently being positive (being excited, being happy), and so the conclusion is that above tweet is considered as a \"positive tweet\".\n",
        "\n",
        "So in our first architecture we'll learn how we can conclude a tweet is either positive or negative by checking every word and see if there are any hints that tweet have either positive, or negative sentiment. For above tweet the hint would be the word \"Excited\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni_bj22seqZP"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "For our learning to classify tweets into it's sentiment, we will use this dataset [https://www.kaggle.com/datasets/ferno2/training1600000processednoemoticoncsv](https://www.kaggle.com/datasets/ferno2/training1600000processednoemoticoncsv). It's a dataset of 1,6 million of tweets that's already classified as either positive tweet for negative tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijsWqXLJzbp"
      },
      "source": [
        "# Preprocessing - Cleaning noises, and consolidating words - Human part before we input to the machine\n",
        "\n",
        "One of the place where human can \"help\" the machine learning model is in preprocessing. One of the task that are done in preprocessing is to make sure that our model won't be distracted by several things that we as a human might figure out that the model shouldn't care about, and transform several things to make sure our model can works better.\n",
        "\n",
        "When we're working on classification (especially when working with model but without neural network), we should consider our goal when we're looking at our dataset: What words do our model really need to consider when classifying our data.\n",
        "\n",
        "As for NLP using something like naive bayes mostly depends on understanding if a tweet contain certain words that can help it's understanding if the tweet is either positive or negative. So there are basicallly two things that we should do before feeding our input to our model:\n",
        "\n",
        "## Removing noises, which is words or characters that shouldn't give any effect in our classification tasks.\n",
        "\n",
        "> Example: ðŸ˜ƒ Super excited to share my latest article! @OpenAI ðŸ‘€ðŸ‘‰ http://ai.newpost.com #AI #OpenAI ðŸ˜Ž\n",
        "\n",
        "If we're currently doing sentiment classification we might not need to include urls, mentions, hashtags, etc. If we include those into our model, our model might hint those noises as something that geared the tweet sentiment towards either positive or negative.\n",
        "\n",
        "Another example for sentiment classification tasks is removing stopwords. Stopwords are words that occur so frequently in sentences that they contain little meaningful information. Examples of common stopwords in the English language include: \"the\", \"is\", \"at\", \"which\", \"on\".\n",
        "\n",
        "Other things that we might considering removing is symbols like \"?\", \"!\", etc.as -at least when we're not using neural network- understanding sentiment from symbols might be cout\n",
        "\n",
        "## Consolidating words that have similar meaning, by removing their tenses, plurality, prefix, suffix, etc.\n",
        "\n",
        "Words like \"Exciting\" is consolidated with \"excited\", \"excitement\", \"excite\", etc. so we can consider words that have the same root (\"Exciting\", \"Excited\", \"Excite\", are have the same root word: \"Excite\") to be processed together.\n",
        "\n",
        "Another thing we might consider is to lowercasing so \"Exciting\", \"exciting\", and \"EXCITING\" can be considered the same so our model won't differentiate between those three words when learning the sentiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGBjMOyb_TKE"
      },
      "source": [
        "# Let's remove all noises\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZBm3aU2I-cQ",
        "outputId": "dd72b420-9f0b-404c-b45a-83742ad7e40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey  check out the webpage  I found it awesome  \n"
          ]
        }
      ],
      "source": [
        "# @title Remove noises\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_hashtags(text):\n",
        "    hashtag_pattern = re.compile(r'#\\S+')\n",
        "    return hashtag_pattern.sub(r'', text)\n",
        "\n",
        "def remove_mentions(text):\n",
        "    mention_pattern = re.compile(r'@\\S+')\n",
        "    return mention_pattern.sub(r'', text)\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_symbols(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def preprocess_sentence(text):\n",
        "    text = remove_urls(text)\n",
        "    text = remove_hashtags(text)\n",
        "    text = remove_mentions(text)\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_symbols(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "text = \"Hey @user, check out the webpage: https://example.com. I found it awesome! ðŸ˜Ž #exciting\" # @param {text: \"string\"}\n",
        "print(preprocess_sentence(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY_iLLgF9-89"
      },
      "source": [
        "# Stemming and lemmatization\n",
        "\n",
        "When consolidating words that have the same root, there are two strategies that can be used: Stemming and lemmatization.\n",
        "\n",
        "## Stemming\n",
        "\n",
        "> Exciting, excited. Happy, happiness. Sad, sadden, sadness. Worrying, worried, worry.\n",
        "\n",
        "The way stemming handle words consolidating is by removing  the suffixes (and sometimes prefixes) of the words, leaving only the word \"stem\" (the part of the word that is common to all its inflected variants). It's easier to learn by example:\n",
        "\n",
        "> exciting -> excite\n",
        "\n",
        "The unique thing about stemming is that it reduce to several characters that are unique to other words, but sometimes it doesn't really \"make sense\" in the meaning of the word. As long as it can manage to group several pattern of the same words as one, lots of task can be enough to use this.\n",
        "\n",
        "> went != go\n",
        "\n",
        "It's only caring about reducing the words to the most basic letter that unique from other words, not caring to their synonyms, tenses, or likewise. For example \"went\" and \"go\" would be different in stem even though \"went\" is just a past tense of \"go\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CriMLC0YCNt"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is different from stemming such that it emphasizes a heavy consideration for grammar rules in its approach. While both methodologies aim to reduce words to their base or root form, lemmatization performs this task by taking into account the morphological analysis of the words. This means that it understands the context and proper grammatical elements such as verb tenses, plural forms, and even gender to extract the correct linguistic base form of a word, known as 'lemma'.\n",
        "\n",
        "> Better -> Good. Geese -> goose. Went -> Go\n",
        "\n",
        "As we can see from above examples, lemmatization profoundly recognizes and accurately transforms words into their dictionary or base form, considering their tenses, their plurality, and more.\n",
        "\n",
        "This can't be achieved with stemming as stemming is merely \"Chopping off\" words rather than considering dictionary at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9PmzEMYZbC5"
      },
      "source": [
        "# Quick library note: NLTK\n",
        "\n",
        "Going forward, we'll use NLTK a lot. NLTK is short for Natural Language Toolkit, a python library that has a lot of functionality to work with NLP in Python. You can use this library for lots of thing such as removing stopwords, tokenizing, stemming, lemmatizing, and more. You can learn more on https://www.nltk.org/ and check what capabilities that this library has by checking https://www.nltk.org/py-modindex.html ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13kgR9JrzurT"
      },
      "source": [
        "## Stemming in practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5uA4L5ZzuOX",
        "outputId": "77c9a537-7825-412c-8320-9ed5a93a6c65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The striped bats were HANGING on their feet and eating best batches of juicy leeches\n",
            "the stripe bat were hang on their feet and eat best batch of juici leech\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Stemming\n",
        "# Import the necessary libraries\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required datasets from nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "text = \"The striped bats were HANGING on their feet and eating best batches of juicy leeches\" #@param {type: \"string\"}\n",
        "\n",
        "# Tokenize the text\n",
        "token_list = word_tokenize(text)\n",
        "\n",
        "# Apply stemming on the tokens\n",
        "stemmed_output = ' '.join([stemmer.stem(token) for token in token_list])\n",
        "\n",
        "print(text)\n",
        "print(stemmed_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSnJjB0Hly4a"
      },
      "source": [
        "# Lemmatization In Practice\n",
        "\n",
        "The process of lemmatization is a little bit more complex than stemming because we need every words \"POS tag\" to make sure that the lemmatization lemmatize to the correct part of speech.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jr_JKvaNGBi"
      },
      "source": [
        "## POS (Part of speech)\n",
        "\n",
        "Part of speech is as simple as asking to each words: Is it a noun? Is it a verb? Is it an adjective? Etc. This helps in making sure that every word converted to the correct lemma.\n",
        "\n",
        "Of course, different from stemming, for lemmatization to work correctly we must ensure that our input still contains stopwords to ensure the POS is correct. So if you want to do lemmatization ensure that POS is done before removing all stopwords, or removing any words at all.\n",
        "\n",
        "Below is the code for lemmatization, feel free to change the input text to any sentence that you want to see lemmatization on play."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgX20i72qjsx",
        "outputId": "ca5a7fcf-7ac5-4d58-ed4e-37c509e89d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The : Others\n",
            "striped : Adjective\n",
            "bats : Noun\n",
            "were : Verb\n",
            "hanging : Verb\n",
            "on : Others\n",
            "their : Others\n",
            "feet : Noun\n",
            "and : Others\n",
            "eating : Verb\n",
            "best : Adjective\n",
            "batches : Noun\n",
            "of : Others\n",
            "juicy : Noun\n",
            "leeches : Noun\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title POS\n",
        "# Import the necessary libraries\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download required datasets from nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_human_readable_pos(treebank_tag):\n",
        "    \"\"\"Map `treebank_tag` to equivalent human readable POS tag.\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return \"Adjective\"\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return \"Verb\"\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return \"Noun\"\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return \"Adverb\"\n",
        "    else:\n",
        "        return \"Others\"\n",
        "\n",
        "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" # @param {text: \"string\"}\n",
        "\n",
        "# Tokenize the text\n",
        "token_list = word_tokenize(text)\n",
        "\n",
        "# POS tagging on the tokens\n",
        "pos_tokens = pos_tag(token_list)\n",
        "\n",
        "# Print word with its POS tag\n",
        "for word, pos in pos_tokens:\n",
        "    print(f\"{word} : {get_human_readable_pos(pos)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1L5txG-Pjz3"
      },
      "source": [
        "## Let's lemmatize\n",
        "\n",
        "Now after POS tagging are done, we can pass the POS tagging along with every words to our lemmatization function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kufBPpGYPmyP",
        "outputId": "4f8ae34e-27cf-44b2-e815-d9eaf60c46e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input The striped bats were hanging on their feet and eating best batches of juicy leeches\n",
            "output The striped bat be hang on their foot and eat best batch of juicy leech\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Lemmatization\n",
        "# Import the necessary libraries\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "import nltk\n",
        "\n",
        "# Download required datasets from nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Map `treebank_tag` to equivalent WordNet POS tag.\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        # As default pos in lemmatization is Noun\n",
        "        return wordnet.NOUN\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"The striped bats were hanging on their feet and eating best batches of juicy leeches\" #@param {text: \"string\"}\n",
        "\n",
        "# Tokenize the text\n",
        "token_list = word_tokenize(text)\n",
        "\n",
        "# POS tagging on the tokens\n",
        "pos_tokens = pos_tag(token_list)\n",
        "\n",
        "# Lemmatize with POS tagging\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tokens])\n",
        "\n",
        "print(\"input\",text)\n",
        "print(\"output\",lemmatized_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4YpPj_VmpT2"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Tokenization is one of the latest part of preprocessing in NLP. The definition is simple: It's a process to breakdown our preprocessed words into array of features that already preprocessed so we can feed it to our process.\n",
        "\n",
        "Why we called it features? For our current architecture, a feature is basically a single pre-processed word. But later when we're using neural networks, a feature might be refer to sub-words.\n",
        "\n",
        "Words such as \"eating\", when we tokenized into sub-words, might be tokenized into something like \"eat-ing\". But sub-words as features mostly held place when we need semantic relation between words, but for learning how NLP works without neural network it's basically harder and mostly we can just refer to neural network for tasks that require these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5FuDTRvkt7Z",
        "outputId": "e032efe7-c416-4b9e-95cc-a8f172a2ef0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', 'for', 'basic', 'tokenization', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# @title Basic tokenization\n",
        "# Import required library\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example sentence for basic tokenization.\" #@param {text:\"string\"}\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Output the tokens\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEp-jMrYlBrw",
        "outputId": "8e5e3e21-8ab4-4db6-e26a-9a0ba7248178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pe', '##mer', '##ik', '##sa', 'ke', '##uan', '##gan', 'mas']\n"
          ]
        }
      ],
      "source": [
        "# @title Sub-words (BPE, byte pair encoding) tokenizer that is used by BERT model\n",
        "# Import required library\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Initialize the tokenizer with a pretrained model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample text\n",
        "text = \"Pemeriksa Keuangan mas\" #@param\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Output the tokens\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sub-words (BPE, byte pair encoding) tokenizer that is used by BERT model id\n",
        "\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForMaskedLM)\n",
        "\n",
        "tokenizer_id = AutoTokenizer.from_pretrained(\n",
        "    \"indolem/indobert-base-uncased\")\n",
        "text = \"badan pemeriksaan keuangan melihat ada hal janggal\" #@param\n",
        "tokens = tokenizer_id.tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "9W4qumWZFR4n",
        "outputId": "72bce271-dcbf-48f8-eab5-2e3ba2533412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['badan', 'pemeriksaan', 'keuangan', 'melihat', 'ada', 'hal', 'janggal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-csQ8_l3ZbC7"
      },
      "source": [
        "As you can see above that some of the word is splitted to sub-words: \"batch + ##es\" and \"lee + ##ches\". What to split to subwords is depend on the task at hand of course, and for BERT cases, lot's of verb still considered a single token rather that splitting it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePUaRM5o48Sq"
      },
      "source": [
        "# How our model will understand which sentiment to assign our tweet to?\n",
        "\n",
        "Let's get a while back and try to understand below tweet:\n",
        "\n",
        "> I'm really excited towards tomorrow for our shop opening, see you guys at xxx!\n",
        "\n",
        "How can we conclude that tweet is positive again? It's because it's having the word **excited**, as we know that the word **excited** are more likely hinting to a sentence that is positive, but is unlikely to be existing on a sentence that is negative.\n",
        "\n",
        "So how can a model know, especially when we're not doing deep-learning, how to differentiate a sentiment of a tweet? By checking if a sentence containing words that give hint towards one of the sentiment\n",
        "\n",
        "## Excited are unlikely to be occuring on negative tweet\n",
        "\n",
        "So how can we teach a machine that certain words should give a great hint that a tweet is positive while certain words can give a great hint for otherwise?\n",
        "\n",
        "We can of course just feed \"excited\", \"happy\", \"sad\", etc, then tag them to be one way or another, but imagine if we don't have the dictionary for all positive words and negative words, how can we compile them?\n",
        "\n",
        "> \"Excited are unlikely to be occuring on negative tweet\"\n",
        "\n",
        "So if we can gather lots of tweets that already tagged as positive and negative, we can compile every word that are positive by checking all positive tweets and if there are lots of tweets that hinting that this word is positive.\n",
        "\n",
        "> \"Word like 'technology' can be occuring on positive and negative tweet, and shouldn't affect a sentiment\"\n",
        "\n",
        "But don't forget that some words are neutral. It depends on your dataset, but let's say if we're scraping tweets from tech reviewer, the word \"technology\" would appear on positive sentiment, while still of course shown on negative sentiment.\n",
        "\n",
        "> So our first formula might be:\n",
        "> - If a word often shown on a sentiment, it might be hinting that it's classify as that sentiment\n",
        "> - But if a word geared towards both sentiment, it most likely hinting that it's a neutral word\n",
        "\n",
        "Above concept will be our baseline to understand two methods of Feature Extraction: Bag-of-words, and TF-IDF, which we'll learn in our next session!\n",
        "\n",
        "## Isn't NLP exciting?\n",
        "\n",
        "There are lots of challenge when it comes to a task as simple as sentiment analysis:\n",
        "\n",
        "- I'm not really interested thanks!\n",
        "\n",
        "For sentence like that, we have to make sure that our model knowing to use that \"not\" and negate anything after\n",
        "\n",
        "- Wow, that was so interesting that I fell asleep in the mid of the event\n",
        "\n",
        "For sarcasm, it's a whole another level. How to solve something like that? How a model can know which sentence is sarcasm, and which are not?\n",
        "\n",
        "Of course we won't give you the answer right away ðŸ˜›, stay tune and stay curious!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "stemming, lemmatization pilih salah satu\n",
        "\n",
        "tokenizer/tf-idf/bag of words pilih salah satu\n",
        "\n",
        "kalau bahasanya jaksel gitu pake model/lemmatization/tokenizer yang mana donk? > model, C4, OSCAR crawler dari Internet"
      ],
      "metadata": {
        "id": "I_77_p_FJfQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "kalau habis ditokenize, apa harus diencode jadi vektor dulu mas?\n",
        "iya"
      ],
      "metadata": {
        "id": "sVhHxmRjLfd6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}