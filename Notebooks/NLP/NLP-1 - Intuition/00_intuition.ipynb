{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Depedencies"
      ],
      "metadata": {
        "id": "cjlmZPUuqJjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wHplEy8jaOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f1c7d1-f15c-4ad3-d4d7-3cedc0480413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_EG6OCq9zf-"
      },
      "source": [
        "# Intuition\n",
        "\n",
        "Welcome to NLP learning, we'll start warming up with just mostly about intuition that you should understand about how natural language processing works, what is preprocessing and how to preprocess our data, then we'll see the intuition about how several architecture works behind the scene, and much more.\n",
        "\n",
        "## The Black Box\n",
        "\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/output_IVkit2X9C.gif?updatedAt=1695021798371)\n",
        "\n",
        "Natural language processing is a concept of understanding \"natural language\" ( which basically means \"human language\") and making sure a machine can understand that \"natural language\". But how can a machine understand humand language, let alone answering it like ChatGPT above?\n",
        "\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_6vrPLsRVx.png?updatedAt=1695020969115)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liADxjsIZr8G"
      },
      "source": [
        "# Machine only understand numbers\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_F42l4buMr.png?updatedAt=1695027219913)\n",
        "\n",
        "We've learned machine learning so far, and what is one of the most oversimplification but kinda true about machine learning? When it comes to inferring, it's basically just a giant calculator that keep doing matrix multiplication, activation, etc.\n",
        "\n",
        "So when we're facing our computer to learn about language, we need to convert them into numbers first. For that later we'll learn about:\n",
        "\n",
        "1. Preprocessing\n",
        "2. Tokenization\n",
        "3. Word embedding\n",
        "\n",
        "After the \"Magic\" process (that of course we'll learn more later), we'll need to convert the output process back to human language. This process of converting numbers back to human language is mostly using fully-connected layer, things that we have learned again and again, not that complex right? üòâ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWhxjtv7ZWjY"
      },
      "source": [
        "# The process of answering\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_XE289cX9B.png?updatedAt=1695027950063)\n",
        "\n",
        "Now let's uncover more \"magic\". In general the magic itself can be divided into two categories: Encoder (The part of where the model try to understanding the question) and decoder (The part where the model finding the answer and converting the answer which previously only can be understand by our modal back to human words).\n",
        "\n",
        "## Encoder\n",
        "\n",
        "Encoder's position in NLP is to \"encode context from human input\", which translates to \"making human language to make it digestable by our model. So for lots of architecture -despite the modern architecture that lacking encoder layer- encoder is a part of \"converting\" input to a vector, array of numbers that can be processed further after that.\n",
        "\n",
        "So a sentence can be summarized into an array of numbers that later can be processed further by the machine, but if zoomed in further even every single words can be summarized into an array of numbers, which is called \"Word embedding\" which we'll learn right after this.\n",
        "\n",
        "A sentence have lots of words, every single words have their own array of numbers that can be understood by a machine, **the encoder is a way to summarize those words into another array of numbers that encapsulate the whole sentence meaning**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Model and Tokenizer\n",
        "from transformers import BartTokenizer, BartModel\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "model = BartModel.from_pretrained('facebook/bart-large')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HZP_v2y-qNnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHpZwIcHVSef",
        "outputId": "542b67e2-5aa0-455c-f033-a689fcfe9fbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.0119,  0.7877, -0.4751,  ...,  1.0580,  0.1460, -0.9939],\n",
              "         [ 1.0119,  0.7877, -0.4751,  ...,  1.0580,  0.1460, -0.9939],\n",
              "         [-0.2349,  3.1772, -1.2153,  ..., -1.5666,  1.0641,  1.4791],\n",
              "         ...,\n",
              "         [ 1.0240,  1.0367, -0.0746,  ...,  0.1491, -0.3696,  0.9102],\n",
              "         [ 0.4020,  0.6007, -1.9825,  ..., -0.1321, -0.2089,  0.3018],\n",
              "         [ 0.3289,  1.5758, -0.4973,  ..., -0.0410,  0.8629,  1.4661]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#@title Encoded text\n",
        "\n",
        "# Input text\n",
        "text = \"tutur tinular\" #@param\n",
        "\n",
        "# Encode text\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# Get embeddings\n",
        "embeddings = model(**encoded_input)['last_hidden_state']\n",
        "\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input[\"input_ids\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tq5guZHLvh4",
        "outputId": "1252e3a1-a697-4c39-82d9-7ab317fb082c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0vRtA5K3eRY",
        "outputId": "d7c21570-84da-49c5-f00e-87c03bed39ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lRfEiTrVYb8"
      },
      "source": [
        "### NLP model == Complex calculator\n",
        "\n",
        "If you run above code, you'll see that above sentence are translated into numbers that can later be processed further into our model. So by seeing array of matrix above hopefully you can be more understand that the basis of NLP is basically lots and lots of matrix operation: It's basically a really complex calculator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXf_7YmIj_de"
      },
      "source": [
        "# Word embedding - Where every words containing a lot of contexts\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AYEy4OlpeyE"
      },
      "source": [
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_fCEXzZbEY.png?updatedAt=1695050596556)\n",
        "\n",
        "Every single words in NLP refer to a word embedding, a word embedding basically an array of numbers that represent the word context, that every single word embedding can contains hundreds or thousands of dimension that are containing different \"context\".\n",
        "\n",
        "So word like \"horse\" for example, have a word embedding that refer to that exact word that more or less look like: \"[0, 3213, 732 ..,943]\".\n",
        "\n",
        "To not be ahead of ourselves, consider when we want to complete below sentence:\n",
        "\n",
        "> Andi walks into his own ___\n",
        "\n",
        "When we're seeing above sentence we might talk more about:\n",
        "- Places\n",
        "- Vehicles\n",
        "\n",
        "So we'll find in words that contains high value in dimension that refer to that. We might find that the word book, glass, microphone, ear, eye, is really unlikely to be the answer of above sentence, as it's not related to places, or vehicles, so the scores should be really low or even negative.\n",
        "\n",
        "But for the words like garden, car, house, would be more likely to complete above sentence, so the score would be higher.\n",
        "\n",
        "So in NLP, multidimensionality refers to every word have their own \"score-card\" which behave like scoring system how a dimension reflect to certain context so when we need an answer we can just \"found them in the referred context\".\n",
        "\n",
        "Of course it's a really high-level intuition which we'll learn more about when we touch word embedding üòÄ\n",
        "\n",
        "> Heads up: In reality word-embedding might not only be a single word have single embedding, but can be divided more to \"sub-words\", which is \"parts-of-words\", a word divided to several parts such as \"hiking\" can be divided to \"hike-ing\".\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Depedencies\n",
        "import os\n",
        "import numpy as np\n",
        "import requests, zipfile, io\n",
        "\n",
        "def download_and_unzip_embeddings(url, directory):\n",
        "    print(f'Downloading and unzipping embeddings...')\n",
        "    r = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(path=directory)\n",
        "\n",
        "def load_glove_embeddings(path, url):\n",
        "    # If file doesn't exist, download and unzip it\n",
        "    if not os.path.isfile(path):\n",
        "        download_and_unzip_embeddings(url, path.rsplit('/', 1)[0])\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "        embeddings = {}\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "        return embeddings\n",
        "\n",
        "# URL of GloVe embeddings and the path - replace with your actual URL\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "path = 'glove.6B/glove.6B.50d.txt'\n",
        "\n",
        "embeddings = load_glove_embeddings(path, url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iLWVikABqW99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CjPUCW1se06",
        "outputId": "3b5dd8a0-ca55-4707-bb2b-4b7afd3e020c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.12099   0.10119  -0.18065   0.20052   0.34682  -1.0604   -1.1647\n",
            "  0.64395  -0.33524  -0.79036  -0.55958  -1.0967   -0.14952   0.36835\n",
            "  0.5777    0.27746  -1.3737   -0.23306  -1.2181   -0.46396   0.91798\n",
            "  1.31      0.5937    0.047051  0.65399  -1.0776    0.33062   1.0118\n",
            "  0.27919   0.15841   3.1537    0.45444  -0.15605   0.33373   0.033046\n",
            " -0.40556   0.41308  -0.28614   0.31554   0.3254   -0.44613   0.36396\n",
            " -0.53107  -0.38338   0.63936  -0.079852  0.04625  -0.20362   0.41999\n",
            "  0.25573 ]\n"
          ]
        }
      ],
      "source": [
        "#@title Get a word embedding\n",
        "import os\n",
        "import numpy as np\n",
        "import requests, zipfile, io\n",
        "\n",
        "# To get a word's embedding\n",
        "word = \"morning\" #@param\n",
        "embedding_vector = embeddings.get(word)\n",
        "\n",
        "# If the word doesn't exist in the dictionary, `get` method will return None.\n",
        "if embedding_vector is not None:\n",
        "    print(embedding_vector)\n",
        "else:\n",
        "    print(f\"'{word}' not found in the dictionary.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Glove > kumpulan word embedding\n",
        "\n",
        "Word embedding > rangkuman atau representasi berupa angka yang didapat dari sebuah algorithm berdasrkan\n",
        "hubungan antar kata\n",
        "\n",
        "- dia bisa ngasih word embedding kalau kata nya ada di dataset awal\n",
        "- dimensi dari embedding < panjang array dari embedding\n",
        "- beda tools beda output"
      ],
      "metadata": {
        "id": "hxBgZnQB12Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pRGiSvVzNPF"
      },
      "source": [
        "As we see above, a \"computer\" can be embedded into numbers that each dimension contain some context of \"computer\". To show above example we use 50 dimension from word embedding named \"GloVe\", but in reality even in GloVe they have variation of number of dimensions like 100 dimensions, 200 dimensions, even 300 dimensions, when the more dimension can contain more context of a word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB68yZWJ77G_"
      },
      "source": [
        "# One word at a time\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/output_IVkit2X9C.gif?updatedAt=1695021798371)\n",
        "\n",
        "When we're talking about question and answering like GPT model, we can see that our input is being answered one at a time.\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_Etk8eVNLg.png?updatedAt=1695051947700)\n",
        "\n",
        "It's simply because decoder mostly work at one-word-at-a-time. So after they got their encoded input, it will process what output that it thinks the best, one-word-at-a-time.\n",
        "\n",
        "After a single word is finished, the model will check if it's the last word or not, if not, then the next decoder process is continue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZTuXHfAYsie"
      },
      "source": [
        "# End-to-end Process Intuition\n",
        "\n",
        "![](https://ik.imagekit.io/ffr6l4jaf5t/REA%20AI/image_Ya5YuwbHF.png?updatedAt=1695076739564)\n",
        "\n",
        "So above diagram is a really high overview of NLP's intuition. Our journey in learning NLP, most of it can be categorized into any of above steps.\n",
        "\n",
        "Remember that some of NLP architecture only use above categorization until \"Undestanding the question\".\n",
        "\n",
        "In more classic architecture the concept of encoder isn't really exist, so the concept of \"Understanding the question\" won't use encoder.\n",
        "\n",
        "In modern architecture there's a concept called \"Self-Attention\" that even can skip the whole concept of \"Encoder\" and just straight to \"Decoder\"."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "<bos> kalimat kita <eos>"
      ],
      "metadata": {
        "id": "0WPOktpj6ZvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tensor, numpy array, list[ 0.12099   0.10119  -0.18065   0.20052   0.34682  -1.0604   -1.1647\n",
        "  0.64395  -0.33524  -0.79036  -0.55958  -1.0967   -0.14952   0.36835\n",
        "  0.5777    0.27746  -1.3737   -0.23306  -1.2181   -0.46396   0.91798\n",
        "  1.31      0.5937    0.047051  0.65399  -1.0776    0.33062   1.0118\n",
        "  0.27919   0.15841   3.1537    0.45444  -0.15605   0.33373   0.033046\n",
        " -0.40556   0.41308  -0.28614   0.31554   0.3254   -0.44613   0.36396\n",
        " -0.53107  -0.38338   0.63936  -0.079852  0.04625  -0.20362   0.41999\n",
        "  0.25573 ]"
      ],
      "metadata": {
        "id": "feUfH2o665eL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}